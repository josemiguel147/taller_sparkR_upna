{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller de procesamiento de BigData en Spark + R\n",
    "Manuel Parra (manuelparra@decsai.ugr.es). <a href=\"http://sci2s.ugr.es/es\">Soft Computing and Intelligent Information Systems</a>\n",
    ". <a href=\"http://sci2s.ugr.es/dicits/\">Distributed Computational Intelligence and Time Series</a>. **University of Granada**.\n",
    "![logos](https://sites.google.com/site/manuparra/home/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando ``spark-submit`` para enviar trabajos al clúster de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El script ``spark-submit`` de Spark se utiliza para iniciar aplicaciones en un clúster sin tener que hacerlo de modo interactivo.\n",
    "\n",
    "En el taller hemos realizado un trabajo interactivo con R. Es posible que sea necesario lanzar un experimento en el clúster sin tener que realizarlo de modo interactivo por las característica propias del desarrollo, como por ejemplo:\n",
    "\n",
    "- Coste en tiempo, alto\n",
    "- Análisis profundos\n",
    "- Baterias de análsis\n",
    "- etc.\n",
    "\n",
    "Esto es importante, pues permite enviar trabajos a la plataforma Spark de una forma cómoda independientemente del lenguaje se se haya utilizado para hacer los experimentos y esperar el resultado cuando se haya realizado completamente el experimento (o ejecución) en el clúster.\n",
    "\n",
    "De modo que se podrán enviar trabajos al clúster en:\n",
    "\n",
    "- Scala\n",
    "- Python\n",
    "- R\n",
    "- Java\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La sintaxis es la siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "spark-submit \\\n",
    "  --class <main-class> \\\n",
    "  --master <master-url> \\\n",
    "  --deploy-mode <deploy-mode> \\\n",
    "  --conf <key>=<value> \\\n",
    "  ... # other options\n",
    "  <application-jar> \\\n",
    "  [application-arguments]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las opciones más usadas son:\n",
    "\n",
    "- **--class** : El punto de entrada de tu aplicación o experimento.\n",
    "- **--master** : La URL del Master de tu cluster. Si es local usamos ``local``\n",
    "- **--deploy-mode** : Desplegar en los nodos ``worker`` o localmente (``local``) como cliente externo.\n",
    "- **--conf** : Configuración genérica de Spark.\n",
    "- **--num-executors** : Número de nodos que ejecutarán el trabajo\n",
    "- **--num-cores** : Número de ``cores`` por nodo ejecutor\n",
    "- **--executor-memory** : Memoria en ``GB`` por utilizada máxima por cada nodo ejecutor\n",
    "\n",
    "<span style=\"color:red\">Este comando se usa directamente desde el shell de la Máquina virtual</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de aplicación en R para ``spark-submit``\n",
    "\n",
    "Este ejemplo recibe dos parámetros desde la línea de comandos, uno con la cantidad (``amount``) y otro con el fichero de salida de los datos (``resultpath``). Conecta con Spark, lee un dataset de ejemplo (``/SparkR/datasets/csv/buy_costumers_amazon01.csv``) y luego filtra ese dataset por la cantidad (``amount``). Finalmente almacena el resultado en la ruta indicada por el parametro de entrada ``resultpath``.\n",
    "\n",
    "Copia el siguiente código en un fichero en R con el nombre ``miaplicacion.R``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Leo los agurmentos\n",
    "args <- commandArgs(trailingOnly =TRUE)\n",
    "\n",
    "# Tomo el parametyos amount para hacer alguna operación\n",
    "amount <- args[1]    \n",
    "resultpath <- args[2]\n",
    "\n",
    "# Fijamos la ruta donde está instalado Spark\n",
    "Sys.setenv(\"SPARK_HOME\"='/usr/local/spark/')\n",
    ".libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"),\"R/lib/\"),.libPaths()))\n",
    "\n",
    "library(SparkR)\n",
    "\n",
    "# Abro la sesion de SparkR\n",
    "sparkR.session(appName=\"EntornoInicio\", master = \"local[*]\", sparkConfig = list(spark.driver.memory = \"1g\"))\n",
    "\n",
    "# Leo un dataset\n",
    "df <- read.df(\"/SparkR/datasets/csv/buy_costumers_amazon01.csv\", \"csv\", header = \"true\", inferSchema = \"true\")\n",
    "\n",
    "# Filtro el dataset por el parametro de entrada a la aplicacion\n",
    "df1 <- filter(df,df$amount>amount)\n",
    "\n",
    "#Escribimos el SparkDataFrame df1\n",
    "write.df(df1, path = resultpath, source = \"csv\", mode = \"overwrite\")\n",
    "\n",
    "sparkR.session.stop() \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución para R en local\n",
    "\n",
    "Para ejecutar una aplicación en R y enviarla como trabajo a Spark, hay que usar la siguiente sentencia:\n",
    "\n",
    "``spark-submit \n",
    "        --deploy-mode <modo> \n",
    "        --master <master> \n",
    "        --num-executors <num> \n",
    "        --executor-memory <mem g> \n",
    "        <aplicacion.R> <parametro1> <parametro2> <parametro3> ...\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark-submit --deploy-mode client  --master local  --num-executors 1  --executor-memory 1g miaplicacion.R 2000.0 /tmp/resultados.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución para R usando Rscript tanto en modo local como en clúster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nohup Rscript miaplicacion.R 2000.0 /tmp/resultados.csv &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En R en clúster\n",
    "\n",
    "Para ejecutar una aplicación en R y enviarla como trabajo a Spark, hay que usar la siguiente sentencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark-submit --deploy-mode cluster  --master spark://HOST:PORT  --num-executors 5  --executor-cores 5  --executor-memory 4g  miaplicacion.R 2000.0 /tmp/resultados.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "<div style=\"font-family:helvetica;padding:5px;font-size:1.5em;background-color:#CFE7E2\">Ejercicio práctico:</div>\n",
    "\n",
    "Crea una aplicación con el nombre ``seleccion.R`` que reciba 4 parámetros:\n",
    "\n",
    "- nombre de la columna: ``colname``\n",
    "- número de registros: ``numrows``\n",
    "- fichero de entrada: ``source_file``\n",
    "- fichero de salida: ``destination_file``\n",
    "\n",
    "Tome el fichero de entrada, extraiga (seleccione: select(...)) sólo la columna ``colname`` y sólo ``numrows`` filas del dataset.\n",
    "El dataset resultante ( de una columna y numrows filas ) debe ser almacenado en el fichero ``destination_file`` en CSV.\n",
    "\n",
    "Prueba como fichero de entrada: ``/SparkR/datasets/databig/ECBDL14_10tst.data``\n",
    "y como fichero de salida: ``/tmp/salida.csv``\n",
    "\n",
    "**Ejecuta la aplicación utilizando ``spark-submit``**\n",
    "\n",
    "Verifica que al terminar de ejecutar el trabajo, aparece el dataset en ``/tmp/salida.csv``.\n",
    "\n",
    "<HR>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
