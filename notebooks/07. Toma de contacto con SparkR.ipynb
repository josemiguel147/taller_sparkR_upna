{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller de procesamiento de BigData en Spark + R\n",
    "Manuel Parra (manuelparra@decsai.ugr.es). <a href=\"http://sci2s.ugr.es/es\">Soft Computing and Intelligent Information Systems</a>\n",
    ". <a href=\"http://sci2s.ugr.es/dicits/\">Distributed Computational Intelligence and Time Series</a>. **University of Granada**.\n",
    "![logos](https://sites.google.com/site/manuparra/home/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer ejemplo  y toma de contacto con SparkR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre para todos nuestros `scripts` con **SparkR**, cargamos la biblioteca, y creamos una nueva sesión de SparkR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fijamos la ruta donde está instalado Spark\n",
    "Sys.setenv(\"SPARK_HOME\"='/usr/local/spark/')\n",
    "\n",
    ".libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"),\"R/lib/\"),.libPaths()))\n",
    "# Añadimos la bibioteca\n",
    "library(SparkR)\n",
    "# Abrimos la conexión\n",
    "sparkR.session(appName=\"Primeros_Pasos\", master = \"local[*]\", sparkConfig = list(spark.driver.memory = \"1g\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con **SparkR** podemos crear un `DataFrame` de Spark desde un `data.frame` habitual usado en R. \n",
    "\n",
    "Un ``DataFrame`` es una colección distribuida de datos organizada en columnas. \n",
    "\n",
    "Los ``dataframes`` son conceptualmente equivalentes a bases de datos relacionales o a `data.frames` en R o Python, pero con una ventaja: son mucho más eficientes para el trabajo con grandes volumenes de datos. Los ``DataFrames``pueden ser creados desde una amplio surtido de fuentes muy diferentes. Es decir, casi cualquier cosa puede ser un ``dataframe``, por ejemplo ficheros estructurados, tablas en ``HIVE``, bases de datos externas o RDDs.\n",
    "\n",
    "\n",
    "Los RDDs son la principal abstracción de datos en Spark. Un RDD es una colección resilente y distribuida de registros. Esta es una de las claves de Spark y es uno de los componentes fundamentales del `core` de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vamos a usar un dataset sencillo integrado en R\n",
    "# El dataset contiene el tiempo de espera entre erupciones y duración \n",
    "# de la erupción de un geiser de Yellowstone\n",
    "class(faithful)\n",
    "\n",
    "# Convertimos un dataframe de R en un DataFrame de Spark, que llamaremos SparkDataFrame\n",
    "df_faithful <- createDataFrame(faithful)\n",
    "\n",
    "# Vemos el tipo de dataset nuevo\n",
    "class(df_faithful)\n",
    "\n",
    "# Visualizamos de forma rápida el contenido\n",
    "head(df_faithful)\n",
    "\n",
    "# Usamos la función printSchema de SparkR para 'deducir' el esquema de datos (la estructura)\n",
    "printSchema(df_faithful)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ``SparkDataFrame`` puede ser registrado como una vista temporal en ``SparkSQL`` y que permite ejecutar sentencias SQL sobre los datos. La funcionalidad de SQL permite a las aplicaciones y flujos de trabajo ejecutar consultas SQL de forma programatica, devolviendo el resultado también como SparkDataFrame.\n",
    "\n",
    "Esto es importante, ya que todas las transformaciones a los conjuntos de datos que están en formato SparkDataFrames, siguen siendo SparkDataFrames, lo que hace que toda su manipulación corra por parte de Spark con todas las ventajas que eso tiene:\n",
    "\n",
    "- Volumen masivo de datos\n",
    "- Almacenamiento distribuido\n",
    "- Resilencia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark es perezo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_iris <- createDataFrame(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos los siguientes trozos de código en R:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p <- proc.time()\n",
    "df_filtrado <- filter(df_iris,df_iris$Species==\"setosa\")\n",
    "proc.time()-p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p <- proc.time()\n",
    "count(df_filtrado)\n",
    "proc.time()-p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar una función de tipo acción (en esta caso ``count``, ``print``, ``head``, etc.) lanza todo los procesos necesarios para conseguir realizar la acción pedida. En este caso sería hacer el filtro y luego contar. Esto se puede ver en Spark UI (la veremos más adelante)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p <- proc.time()\n",
    "count(df_filtrado)\n",
    "proc.time()-p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hace exactamente lo mismo, primer filtra y luego cuenta, ya que por defecto no hace persistente el DataFrame intermedio aunque lo estemos definiendo como ``df_filtrado``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos hacer persistente un DataFrame intermedio podemos con la función de cacheado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones sencillas con SparkR sobre SparkDataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En estos ejemplos vamos a tratar de ver una parte muy muy simple sobre la manipulación de los datos en el formato que entiende SparkR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Contamos los elementos a partir de un filtro normal\n",
    "count(filter(df_faithful,\"eruptions>3.0\"))\n",
    "\n",
    "# Convertimos a vista temporal de datos en SparkSQL y le damos el nombre faithful a la 'tabla'\n",
    "createOrReplaceTempView(df_faithful,\"faithful\")\n",
    "\n",
    "# Usamos SparkSQL para hacer consultas a los datos.\n",
    "eruptions_sql <- sql(\"SELECT eruptions FROM faithful WHERE eruptions >= 3.0\")\n",
    "\n",
    "# Contamos el resultado\n",
    "count(eruptions_sql)\n",
    "\n",
    "# Mostramos un resumen\n",
    "head(eruptions_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones sobre los conjuntos de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La guía de referencia de funciones de la API de Spark con R se puede ver en: \n",
    "\n",
    "    https://spark.apache.org/docs/2.0.0-preview/api/R/\n",
    "\n",
    "Veamos las más importantes y sus diferencias con las de R equivalentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordamos que siempre para trabajar con SparkR, tenemos que terminar la sesión de Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfaces de gestión de trabajos de Spark\n",
    "\n",
    "Spark pone a disposición del administrador un sistema web que permite controlar el estado del cluster en cuando a jobs, workers, operaciones, ...\n",
    "\n",
    "\n",
    "Gestión de nodos Workers:\n",
    "- En http://192.168.99.10:8080 \n",
    "\n",
    "Gestión de Jobs:\n",
    "- En http://192.168.99.10:4040/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkR.session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta sentencia se cierra el contexto abierto en SparkR y se liberan todos los recursos."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
