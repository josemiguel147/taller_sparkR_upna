{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller de procesamiento de BigData en Spark + R\n",
    "Manuel Parra (manuelparra@decsai.ugr.es). <a href=\"http://sci2s.ugr.es/es\">Soft Computing and Intelligent Information Systems</a>\n",
    ". <a href=\"http://sci2s.ugr.es/dicits/\">Distributed Computational Intelligence and Time Series</a>. **University of Granada**.\n",
    "![logos](https://sites.google.com/site/manuparra/home/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning con ``sparklyr```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biblioteca de Machine Learning Spark (MLlib) con la Interfaz sparklyr\n",
    "\n",
    "**Carácteristicas fundamentales:**\n",
    "\n",
    "La bilbioteca sparklyr proporciona enlaces a la biblioteca de ML distribuida de Spark. En particular, sparklyr le permite acceder a las rutinas de ML proporcionadas por el paquete spark.ml de Spark. Además junto con la interfaz dplyr de sparklyr, puede crear y afinar fácilmente los flujos de trabajo de ML en Spark, orquestados enteramente dentro de R.\n",
    "Sparklyr proporciona tres familias de funciones que puede utilizar con el aprendizaje de máquina Spark:\n",
    "\n",
    "- Algoritmos de aprendizaje automático para el análisis de datos (funciones  ml_*)\n",
    "- Transformadores de características para manipular características individuales (funciones ft_*)\n",
    "- Funciones para manipular Spark DataFrames (funciones sdf_*)\n",
    "\n",
    "El flujo de trabajo para el análisis de datos con sparklyr podría estar compuesto de las siguientes etapas:\n",
    "\n",
    "- Realizar consultas SQL a través de la interfaz sparklyr dplyr,\n",
    "- Utilizar la familia de funciones sdf_ y ft_ para generar nuevas columnas o particionar su conjunto de datos,\n",
    "- Elegir un algoritmo de aprendizaje automático apropiado de la familia de funciones ml_ * para modelar los datos,\n",
    "- Inspeccionar la calidad del ajuste de su modelo y usarlo para hacer predicciones con nuevos datos,\n",
    "- Recopilar los resultados para la visualización y análisis posterior en R\n",
    "\n",
    "En esta sección vamos a trabajar con las herramientas que proporciona la biblioteca sparklyr para Machine Learning.\n",
    "Como vamos a ver, la funcionalidad es muy similar a la de la biblioteca SparkR, aunque cambia el nombre de las funciones y varios extras más.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialización del entorno\n",
    "\n",
    "Es necesario reiniciar Spark para poder trabajar con esta sesión de sparklyr     \n",
    "\n",
    "<p><span style=\"background-color:red;color:white\">&nbsp; &nbsp; Es necesario reiniciar Spark para poder trabajar con esta sesión de sparklyr &nbsp; &nbsp;</span></p>\n",
    "\n",
    "Ahora para conectar con Spark y abrir una sesión usaremos la siguiente sintaxis (simiar a la del paquete SparkR aunque con ligeras diferencias):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usamos la libreria sparklyr y dplyr.\n",
    "\n",
    "# Ajustar el nivel de visualización de errores !\n",
    "options(warn=0)\n",
    "\n",
    "# Incluimos la bilbioteca de sparklyr\n",
    "library(sparklyr)\n",
    "# Usamos la biblioteca para el manejo de los datos.\n",
    "library(dplyr)\n",
    "\n",
    "# Abrimos la conexión. Importante indicar la versión de Spark que tenemos instalada. En nuestro caso tenemos la 2.0.1\n",
    "sc <- spark_connect(master = \"local\", version = \"2.0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos conectarnos a un entorno Spark completo dentro de un cluster usaríamos la siguiente información:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Abrimos la conexión ahora a un clúster\n",
    "sc <- spark_connect(master = \"spark://HOST:PORT\", version = \"2.0.1\")\n",
    "\n",
    "# Donde HOST y PORT deben ser indicados tal y como el adminstrador de clúster tenga establecido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parada de la sesion con Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Paramos la sesion y el contexto de Spark, para liberar recursos\n",
    "spark_disconnect(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos de ML para el análisis de datos\n",
    "\n",
    "Con Spark + R y la biblioteca sparklyr se puede orquestar la ejecución de varios algoritmos de ML en un cluster con Spark.\n",
    "\n",
    "Estas funciones de ML, conectan directamente con la API de Spark, por lo que están vinculadas a la librería MLLib de Spark.\n",
    "\n",
    "La biblioteca tiene mayor número de funciones para la minería de datos que la propia de SparkR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Contiene los siguientes métodos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alternating Least Squares (ALS) matrix factorization\n",
    "- Decision Trees\n",
    "- Generalized Linear Regression\n",
    "- Gradient-Boosted Tree\n",
    "- K-Means Clustering\n",
    "- Latent Dirichlet Allocation\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- Multilayer Perceptron\n",
    "- Naive-Bayes\n",
    "- One vs Rest\n",
    "- PCA (Principal Components Analysis)\n",
    "- Random Forests\n",
    "- Survival Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cada uno de ellos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Least Squares (ALS) matrix factorization\n",
    "\n",
    "Realiza la factorización de matriz de mínimos cuadrados alternativos sobre un Spark DataFrame.\n",
    "\n",
    "```\n",
    "ml_als_factorization(x, rating.column = \"rating\", user.column = \"user\",\n",
    "  item.column = \"item\", rank = 10L, regularization.parameter = 0.1,\n",
    "  implicit.preferences = FALSE, alpha = 1, nonnegative = FALSE,\n",
    "  iter.max = 10L, ml.options = ml_options(), ...)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Realiza una regresión o clasificación usado árboles de decisión:\n",
    "\n",
    "```\n",
    "ml_decision_tree(x, response, features, max.bins = 32L, max.depth = 5L,\n",
    "  type = c(\"auto\", \"regression\", \"classification\"),\n",
    "  ml.options = ml_options(), ...)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Regression\n",
    "\n",
    "Realiza regresión línearl generalizada sobre un SparkDataFrame\n",
    "\n",
    "```\n",
    "ml_generalized_linear_regression(x, response, features, intercept = TRUE,\n",
    "  family = gaussian(link = \"identity\"), iter.max = 100L,\n",
    "  ml.options = ml_options(), ...)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-Boosted Tree\n",
    "\n",
    "Aplica el algoritmo GBT sobre un SparkDataFrame\n",
    "\n",
    "```\n",
    "ml_gradient_boosted_trees(x, response, features, max.bins = 32L,\n",
    "  max.depth = 5L, type = c(\"auto\", \"regression\", \"classification\"),\n",
    "  ml.options = ml_options(), ...)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "Aplica el algortimo K-Means para clustering a un SparkDataFrame\n",
    "\n",
    "```\n",
    "ml_kmeans(x, centers, iter.max = 100, features = dplyr::tbl_vars(x),\n",
    "  compute.cost = TRUE, tolerance = 1e-04, ml.options = ml_options(), ...)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "Ajusta un modelo LDA a un SparkDataFrame\n",
    "\n",
    "```\n",
    "ml_lda(x, features = dplyr::tbl_vars(x), k = length(features),\n",
    "  alpha = (50/k) + 1, beta = 0.1 + 1, ml.options = ml_options(), ...)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Aplica una regresión lineal a los datos de un SparkDataFrame\n",
    "\n",
    "```\n",
    "ml_linear_regression(x, response, features, intercept = TRUE, alpha = 0,\n",
    "  lambda = 0, iter.max = 100L, ml.options = ml_options(), ...)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Aplica una regresión logística a los datos de un SparkDataFrame\n",
    "\n",
    "```\n",
    "ml_logistic_regression(x, response, features, intercept = TRUE, alpha = 0,\n",
    "  lambda = 0, iter.max = 100L, ml.options = ml_options(), ...)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "\n",
    "Crea y entrena Percetron Multicapa para un SparkDataFrame:\n",
    "\n",
    "```\n",
    "ml_multilayer_perceptron(x, response, features, layers, iter.max = 100,\n",
    "  seed = sample(.Machine$integer.max, 1), ml.options = ml_options(), ...)\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive-Bayes\n",
    "\n",
    "Aplica regresión o clasificación utilizando Naive-Bayes:\n",
    "\n",
    "```\n",
    "ml_naive_bayes(x, response, features, lambda = 0, ml.options = ml_options(),\n",
    "  ...)\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One  vs Rest\n",
    "\n",
    "Aplica regresión o clasificación utilizando O vs R:\n",
    "\n",
    "```\n",
    "ml_one_vs_rest(x, classifier, response, features, ml.options = ml_options(),\n",
    "  ...)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Aplica PCA - Principal Components Analysis:\n",
    "\n",
    "```\n",
    "ml_pca(x, features = dplyr::tbl_vars(x), ml.options = ml_options(), ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest\n",
    "\n",
    "Aplica Regresión o clasificación utilizando el algoritmo de RandomForest:\n",
    "\n",
    "```\n",
    "ml_random_forest(x, response, features, max.bins = 32L, max.depth = 5L,\n",
    "  num.trees = 20L, type = c(\"auto\", \"regression\", \"classification\"),\n",
    "  ml.options = ml_options(), ...)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilidades y extensiones para Machine Learning\n",
    "\n",
    "``sparklyr`` ofrece adicionalmente una serie de funciones para interactuar con los modelos ajustados de Spark ML así como operaciones con los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ``ml_binary_classification_eval``.  Binary Classification Evaluator\n",
    "\n",
    "- ``ml_classification_eval``. Spark ML - Classification Evaluator\n",
    "\n",
    "- ``ml_tree_feature_importance Spark ML`` - Feature Importance for Tree Models\n",
    "\n",
    "- ``ml_saveload`` ,  ``ml_load ml_save`` - Save / Load a Spark ML Model Fit\n",
    "\n",
    "- ``ml_create_dummy_variables``. Create Dummy Variables\n",
    "\n",
    "- ``ml_model`` Create an ML Model Object\n",
    "\n",
    "- ``ml_options`` Options for Spark ML Routines\n",
    "\n",
    "- ``ml_prepare_dataframe``. Prepare a Spark DataFrame for Spark ML Routines\n",
    "\n",
    "- ``ml_prepare_response_features_intercept`` ,  ``ml_prepare_inputs`` , ``ml_prepare_features``. Pre-process the Inputs to a Spark ML Routine\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplos y pruebas con ML de ``sparklyr```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparamos el dataset**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cargamos el dataset a SparkDataFrame:\n",
    "heart <- spark_read_csv(sc, \n",
    "                       name=\"heart\", \n",
    "                       path=\"/SparkR/datasets/BNGhearth.dat\", \n",
    "                       delimiter = \",\", \n",
    "                       header=TRUE,\n",
    "                       overwrite = TRUE)\n",
    "\n",
    "\n",
    "# Creamos un test y un train:\n",
    "partitions_heart <- sdf_partition(heart,training = 0.8, test = 0.2, seed = 1099)\n",
    "\n",
    "# Contamos las particiones:\n",
    "count(partitions_heart$test)\n",
    "count(partitions_heart$training)\n",
    "\n",
    "# Imprimimos un resumen para recordar las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(partitions_heart$test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# La función para la regresión lineal puede usarse con la sintaxis propia \n",
    "# de la función y tambien con la forma tradicional de R: formulae\n",
    "\n",
    "# Opción 1\n",
    "#model <- partitions$training %>%\n",
    "#    ml_linear_regression(response = \"f1\", features = c(\"f2\",\"f3\"))\n",
    "\n",
    "# Opción 2\n",
    "#model <- partitions$training %>%\n",
    "#    ml_linear_regression(f1~f2+f3)\n",
    "\n",
    "# Opción 3\n",
    "model <- ml_linear_regression(partitions_heart$training,bloodpressure~age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos la calidad del ajuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vemos la calidad del ajuste ...\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utilizamos Predict\n",
    "\n",
    "predicted <- predict(model, newdata = partitions_heart$test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aplica la función de regresión logística\n",
    "ml_log <- partitions_heart$training %>%\n",
    "             ml_logistic_regression(response = \"class\", features = c(\"age\",\"sex\",\"bloodpressure\"))\n",
    "\n",
    "# O también equivalente\n",
    "# ml_log <- ml_logistic_regression(partitions_heart$training, response = \"class\", features = c(\"age\",\"sex\",\"bloodpressure\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary(ml_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Medias (K-Means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means es un método de agrupamiento, que tiene como objetivo la partición de un conjunto de n observaciones en k grupos en el que cada observación pertenece al grupo cuyo valor medio es más cercano.\n",
    "Forma parte de las técnicas de clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Equivalente:\n",
    "# model <- ml_kmeans(select(iris_tbl,Petal_Width, Petal_Length),centers = 3)\n",
    "\n",
    "model <- partitions_heart$test %>%\n",
    "          select(age, bloodpressure) %>%\n",
    "          ml_kmeans(centers = 3)\n",
    "\n",
    "print (model)\n",
    "\n",
    "# Cuidado con collect !!!\n",
    "partitions_heart$test %>%\n",
    "  select(age, bloodpressure) %>%\n",
    "  collect %>%\n",
    "  ggplot(aes(age, bloodpressure)) +\n",
    "    geom_point(data = model$centers, aes(age, bloodpressure), size = 60, alpha = 0.1) +\n",
    "    geom_point(aes(age, bloodpressure), size = 2, alpha = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted <- sdf_predict(model,partitions_heart) %>% collect\n",
    "\n",
    "table(predicted$class,predicted$prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest también conocido en castellano como '\"Selvas Aleatorias\"' es una combinación de árboles predictores tal que cada árbol depende de los valores de un vector aleatorio probado independientemente y con la misma distribución para cada uno de estos. Es una modificación sustancial de bagging que construye una larga colección de árboles no correlacionados y luego los promedia. Se usa para regresión y para clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ml_rf <- ml_random_forest(partitions_heart$training,response=\"class\",features=c(\"age\",\"bloodpressure\", \"chest\"),type=\"classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ml_rf)\n",
    "\n",
    "res_predict <- sdf_predict(ml_rf, partitions_heart$test)\n",
    "\n",
    "res_predict_2 <- collect(ft_string_indexer(sdf_predict(ml_rf, partitions_heart$test),\"class\",\"class_idx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table(res_predict_2$class_idx,res_predict_2$prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(select(res_predict,age,bloodpressure,chest,class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_imp <- ml_tree_feature_importance(sc, ml_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features <- as.character(feature_imp[1:3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Análisis de componentes principales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En estadística, el análisis de componentes principales es una técnica utilizada para reducir la dimensionalidad de un conjunto de datos.\n",
    "Técnicamente, el PCA busca la proyección según la cual los datos queden mejor representados en términos de mínimos cuadrados. Esta convierte un conjunto de observaciones de variables posiblemente correlacionadas en un conjunto de valores de variables sin correlación lineal llamadas componentes principales.\n",
    "El PCA se emplea sobre todo en análisis exploratorio de datos y para construir modelos predictivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca_model <- ml_pca(partitions_heart$training)\n",
    "\n",
    "print(pca_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Comparación de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hacemos una lista para verificar que modelo nos da mejores resultados.\n",
    "ml_models <- list(\n",
    "  \"Logistic\" = ml_log,\n",
    "#  \"Decision Tree\" = ml_dt,\n",
    "  \"Random Forest\" = ml_rf\n",
    "#  \"Gradient Boosted Trees\" = ml_gbt,\n",
    "#  \"Naive Bayes\" = ml_nb,\n",
    "#  \"Neural Net\" = ml_nn\n",
    ")\n",
    "\n",
    "# Create a function for scoring\n",
    "score_test_data <- function(model, data=partitions$test){\n",
    "  pred <- sdf_predict(model, data)\n",
    "  select(pred, class, prediction)\n",
    "}\n",
    "\n",
    "# Score all the models\n",
    "ml_score <- lapply(ml_models, score_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cierre de la conexión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Obligatorio al terminar la session\n",
    "spark_disconnect(sc)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
