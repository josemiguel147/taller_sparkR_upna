{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller de procesamiento de BigData en Spark + R\n",
    "Manuel Parra (manuelparra@decsai.ugr.es). <a href=\"http://sci2s.ugr.es/es\">Soft Computing and Intelligent Information Systems</a>\n",
    ". <a href=\"http://sci2s.ugr.es/dicits/\">Distributed Computational Intelligence and Time Series</a>. **University of Granada**.\n",
    "![logos](https://sites.google.com/site/manuparra/home/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones sobre SparkDataFrames:  ``pipes`` y ``magrittr``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre para todos nuestros `scripts` con **SparkR**, cargamos la biblioteca, y creamos una nueva sesión de SparkR.\n",
    "\n",
    "En este caso:\n",
    "\n",
    "<span style=\"background-color:red;color:white\">&nbsp; &nbsp; Cuidado con la cantidad de MEMORIA que usamos para esta sección ! &nbsp; &nbsp; </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fijamos la ruta donde está instalado Spark\n",
    "Sys.setenv(\"SPARK_HOME\"='/usr/local/spark/')\n",
    "\n",
    ".libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"),\"R/lib/\"),.libPaths()))\n",
    "library(SparkR)\n",
    "sparkR.session(appName=\"EntornoInicio\", master = \"local[*]\", sparkConfig = list(spark.driver.memory = \"1g\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los ``SparkDataFrames`` soportan un alto número de funciones para hacer un procesado de datos estructurado. \n",
    "\n",
    "Vamos a poner en práctica las más utilizadas. \n",
    "\n",
    "**La lista completa de operaciones que se pueden aplicar se puede ver desde API de SparkR en https://spark.apache.org/docs/latest/api/R/index.html **\n",
    "\n",
    "![funcSparkR](https://sites.google.com/site/manuparra/home/functionSparkR.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones con SparkDataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Primero cargamos los datos\n",
    "df_nyctrips <- read.df(\"/SparkR/datasets/yellow_tripdata_2016-02_small1.csv\", \"csv\", header = \"true\", inferSchema = \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hacemos una copia del SparkDataFrame para usarla en una vista temporal en SQL\n",
    "createOrReplaceTempView(df_nyctrips,\"slqdf_filtered_nyc\")\n",
    "\n",
    "# Hacemos una selección de los registros, donde calculamos el tiempo del viaje de cada viaje\n",
    "sql_nyc <- sql(\"select VendorID,INT(unix_timestamp(tpep_dropoff_datetime)- unix_timestamp(tpep_pickup_datetime)) AS trip_time,passenger_count,trip_distance,total_amount from slqdf_filtered_nyc\")\n",
    "\n",
    "head(sql_nyc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Uso de magrittr para el trabajo con los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paquete ``magrittr`` permite: \n",
    "\n",
    "* mejorar el tiempo de desarrollo y \n",
    "* mejorar enormemente la legibilidad y mantenibilidad del código. \n",
    "\n",
    "Para usarlo hay que importar la biblioteca ``magrittr`` dentro del proyecto y apartir de ese momentos podemos utilizar el operador \n",
    "\n",
    "```\n",
    "%>%\n",
    "``` \n",
    "\n",
    "para concaternar operaciones y poder trabajar con flujos de datos y pipelines.\n",
    "\n",
    "Provee de un operador que sirve para hacer `pipes` con el cual se puede `encauzar` un valor hacia adelante dentro de una expresión o llamada a función.\n",
    "\n",
    "Veamos todas las operaciones que hemos realizado sobre los datos y su equivalente con `pipes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usamos magrittr\n",
    "library(magrittr)\n",
    "\n",
    "# results <- sql(\"select VendorID, MAX(trip_distance) from slqdf_filtered_nyc GROUP BY VendorID \")\n",
    "#summarize(groupBy(df_nyctrips, df_nyctrips$passenger_count), count = n(df_nyctrips$passenger_count))\n",
    "\n",
    "df_nyctrips %>% \n",
    "        groupBy( df_nyctrips$passenger_count) %>%\n",
    "        summarize(count = n(df_nyctrips$passenger_count)) %>%\n",
    "        head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_nyctrips %>% \n",
    "        groupBy( df_nyctrips$passenger_count) %>%\n",
    "        summarize( avg_total_amount=avg(df_nyctrips$total_amount) ,avg_trip_distance=avg(df_nyctrips$trip_distance)) %>%\n",
    "        head()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "<div style=\"font-family:helvetica;padding:5px;font-size:1.5em;background-color:#CFE7E2\">Ejercicio práctico:</div>\n",
    "\n",
    "¿Cómo sería la sentencia del bloque superior en SparkSQL?\n",
    "\n",
    "<HR>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_nyctrips %>% \n",
    "         groupBy( df_nyctrips$passenger_count) %>%\n",
    "        summarize(min = min(df_nyctrips$trip_distance),max = max(df_nyctrips$trip_distance)) %>%\n",
    "        head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_nyctrips %>% \n",
    "         groupBy( df_nyctrips$passenger_count, hour(df_nyctrips$tpep_pickup_datetime)) %>%\n",
    "        summarize(total_pickup = n(df_nyctrips$tpep_pickup_datetime)) %>%\n",
    "        head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count(df_nyctrips)\n",
    "\n",
    "num_regs <- as.integer(count(df_nyctrips))\n",
    "\n",
    "# Mostramos el número de registros\n",
    "print(num_regs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<HR>\n",
    "<div style=\"font-family:helvetica;padding:5px;font-size:1.5em;background-color:#CFE7E2\">Pregunta:</div>\n",
    "\n",
    "Haz un flujo de funciones con ``magrittr`` donde selecciones las columnas ``total_amount`` y ``passenger_count``, y filtres cuando ``total_amount`` sea mayor de ``50.0``\n",
    "\n",
    "<HR>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# El quivalente al codigo del bloque anterior\n",
    "\n",
    "sql_nyc %>% count()\n",
    "\n",
    "num_regs <- sql_nyc %>% count() %>% as.integer()\n",
    "\n",
    "num_regs %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<HR>\n",
    "<div style=\"font-family:helvetica;padding:5px;font-size:1.5em;background-color:#CFE7E2\">Pregunta:</div>\n",
    "\n",
    "¿Cuál es la mejor opción para trabajar: ``pipes``, ``SparkSQL`` o funciones?\n",
    "\n",
    "<HR>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
