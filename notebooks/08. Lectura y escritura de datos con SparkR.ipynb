{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller de procesamiento de BigData en Spark + R\n",
    "Manuel Parra (manuelparra@decsai.ugr.es). <a href=\"http://sci2s.ugr.es/es\">Soft Computing and Intelligent Information Systems</a>\n",
    ". <a href=\"http://sci2s.ugr.es/dicits/\">Distributed Computational Intelligence and Time Series</a>. **University of Granada**.\n",
    "![logos](https://sites.google.com/site/manuparra/home/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura y escritura de datos desde SparkR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre, para todos nuestros `scripts` con **SparkR**, cargamos la biblioteca, y creamos una nueva sesión de SparkR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fijamos la ruta donde está instalado Spark\n",
    "Sys.setenv(\"SPARK_HOME\"='/usr/local/spark/')\n",
    "\n",
    ".libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"),\"R/lib/\"),.libPaths()))\n",
    "library(SparkR)\n",
    "sparkR.session(appName=\"EntornoInicio\", master = \"local[*]\", sparkConfig = list(spark.driver.memory = \"1g\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoy en día el trabajo con BigData parece que siempre está asociado al ecosistema HADOOP. Hace unos años esto significaba que si también eras un buen programador en JAVA, trabajar en tal entorno, incluso un simple programa para hacer un WORDCOUNT, implicaba varias docenas de líneas de código. \n",
    "\n",
    "Pero hace 3-4 años la cuestión ha cambiado gracias a Apache Spark con su API de estilo funcional. \n",
    "\n",
    "Está escrito en SCALA pero también puede ser utilizado desde Python, JAVA y como estais viendo por este Taller: también en R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuentes de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro de una sesión de Spark, las aplicaciones pueden crear `SparkDataFrames` desde variadas fuentes de datos, como por ejemplo:  un fichero local (`data.frame`), desde HDFS (``hdfs:///``),  desde tablas en `HIVE` o desde otras múltiples fuentes de datos (AmazonS3, etc).\n",
    "\n",
    "Concretamente las principales fuentes u orígenes de datos desde las que **cargar datos** son los siguientes:\n",
    "\n",
    "* Ficheros locales\n",
    "* Ficheros en sistemas distribuidos de almacenamiento Hadoop HDFS\n",
    "* Sistemas de almacenamiento de datos tipo HIVE\n",
    "* Desde bases de datos relacionales a través de JDBC\n",
    "* ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipos de fuentes de datos\n",
    "\n",
    "Una cosa son las **fuentes de datos** y otras cosa son los **tipo de fuentes de datos**. El tipo de fuente de datos puede ser visto como el formato de los datos.\n",
    "\n",
    "Los conjuntos de datos pueden están almacenados en diferentes formatos, los más utilizados para SparkR (y Spark):\n",
    "\n",
    "* Ficheros planos y CSV\n",
    "* **Ficheros JSON**\n",
    "* Ficheros de tipo ```avro``` (row-based)\n",
    "* Ficheros de tipo ```parquet``` (column-based)\n",
    "* Ficheros de tipo ```orc``` (column-based)\n",
    "\n",
    "![ListOfSources](https://sites.google.com/site/manuparra/home/files_API.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repositorio de Datasets para todo el taller\n",
    "\n",
    "Todos los conjuntos de datos que vamos a tratar para el Taller se encuentran disponibles en el directorio ```datasets```. Para consultar, modificar y añadir datasets, ficheros, etc, puedes hacerlo usando el gestor de fichero de ```Jupyter```.\n",
    "\n",
    "Conjuntos de datos disponibles [aquí](./Datasets del Taller de SparkR.ipynb).\n",
    "\n",
    "<BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajo con ficheros en formato CSV \n",
    "\n",
    "Vamos a revisar todas las funcionalidades que ofrece SparkR para el manejo de CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que hay en el directorio donde tenemos los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list.files(\"/SparkR/datasets/\",full.names = T,recursive = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Leemos un fichero concreto de datos en formato ```CSV``` del directorio ```datasets/csv```. El fichero de ejemplo sólo tiene 1000 registros:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la lectura de datos con SparkR usamos la función ``read.df( )``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sólo indicamos un fichero concreto .... No hay problema Spark es muy listo ! ;)\n",
    "df <- read.df(\"/SparkR/datasets/csv/buy_costumers_amazon01.csv\", \"csv\", header = \"true\", inferSchema = \"true\")\n",
    "printSchema(df)\n",
    "count(df)\n",
    "head(df)\n",
    "\n",
    "df1 <- filter(df,df$amount>1000.0)\n",
    "\n",
    "write.df(df1, path = \"/tmp/df_full.csv\", source = \"csv\", mode = \"overwrite\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si los datasets se encuentran almacenados en HDFS, S3, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Si el fichero está en HDFS o S3Amazon\n",
    "\n",
    "#df <- read.df(\"hdfs://users/datasets/csv/buy_costumers_amazon01.csv\", \"csv\", header = \"true\", inferSchema = \"true\")\n",
    "\n",
    "#df <- read.df(\"s3://SparkR/datasets/csv/buy_costumers_amazon01.csv\", \"csv\", header = \"true\", inferSchema = \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sólo indicamos un fichero concreto .... No hay problema Spark es muy listo ! ;)\n",
    "df <- read.df(\"/SparkR/datasets/csv/buy_costumers_amazon01.csv\", \"csv\", header = \"true\", inferSchema = \"true\")\n",
    "print(\"Estructura sin parsear:\")\n",
    "printSchema(df)\n",
    "\n",
    "# Creamos un esquema para definir cual será la estructura del fichero a leer.\n",
    "schema_amazon <- structType(structField(\"id\", \"integer\"),\n",
    "                     structField(\"first_name\", \"string\"),\n",
    "                     structField(\"last_name\", \"string\"),\n",
    "                     structField(\"buy_hours\", \"string\"),\n",
    "                     structField(\"amount\", \"double\"),\n",
    "                     structField(\"credit_card\", \"string\"))\n",
    "\n",
    "df <- read.df(\"/SparkR/datasets/buy_costumers_amazon01.csv\", \"csv\", header = \"true\", schema=schema_amazon)\n",
    "print(\"Estructura parseada:\")\n",
    "printSchema(df)\n",
    "head(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos leer todos los ficheros de un directorio sin entrar en los subdirectorios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Esto leería todos los ficheros de la carpeta pero no entraría a cada subdirectorio... Spark no eres muy listo !\n",
    "df <- read.df(\"/SparkR/datasets/csv/\", \"csv\", header = \"true\", inferSchema = \"true\", schema=schema_amazon)\n",
    "count(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que estructura de ficheros y directorios tenemos. Observamos que en ```datasets/csv/``` existen subdirectorios, por lo que hay que usar comodines: *****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list.files(\"/SparkR/datasets/csv\",full.names = T,recursive = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos la siguiente instrucción para poder leer todo lo que cuelgue del directorio donde están los CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Leemos todos los ficheros CSV que haya en el directorio y todo en las subcarpetas... Spark que bien, no?\n",
    "df_full <- read.df(\"/SparkR/datasets/csv/*/\", \"csv\", header = \"true\", inferSchema = \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Verificamos que ahora tenemos todos los datos cargados desde todos los ficheros ```CSV``` de la estructura de directorios:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count(df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escritura\n",
    "\n",
    "Una vez que hemos realizado transformaciones con los datos del SparkDataFrame, podemos dejarlo en memoria o bien pasarlo a DISCO (local) o HDFS (distribuido).\n",
    "\n",
    "La API de fuentes de datos puede también ser usada para guardar y almacenar ``SparkDataFrames`` en múltiples formatos. Por ejemplo podemos almacenar el ``SparkDataDrame`` desde/hacia otros formatos como ``CSV``, ``PARQUET`` usando la función ``write.df``. \n",
    "\n",
    "Esto da mucha versatilidad, ya que independiente del tipo de fuente, podemos almacenarlo y leerlo desde cualquiera otra fuente. Como no podía ser de otra forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Escritura desde CSV a CSV:\n",
    "write.df(df_full, path = \"datasets/results/df_full.csv\", source = \"csv\", mode = \"overwrite\")\n",
    "\n",
    "# Escritura desde CSV a PARQUET\n",
    "write.df(df_full, path = \"datasets/results/df_full.parquet\", source = \"parquet\", mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ``mode``\tpodemos usar 'append', 'overwrite', 'error', 'ignore'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PARQUET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet es un formato de **almacenamiento en columnas** disponible para cualquier proyecto dentro del ecosistema de Hadoop, enfocado en la mejora del procesamiento de datos, modelado de datos y programación.\n",
    "\n",
    "Parquet está diseñado para soportar esquemas de compresión y codificación muy eficientes. Múltiples proyectos han demostrado el impacto en el rendimiento de aplicar el correcto sistema de compresión y codificación a los datos. Parquet permite que los esquemas de compresión se especifiquen a nivel de columna.\n",
    "\n",
    "Es un formato bien estructurado para ser usado **para problemas de BigData**.\n",
    "\n",
    "La estructura del fichero se **segmenta en N columnas partidas en M grupos de filas**:\n",
    "\n",
    "![Alt](https://raw.github.com/Parquet/parquet-format/master/doc/images/FileLayout.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos el dataset en formato Parquet, luego el resultado de la lectura es un SparkDataFrame, compatible con el trabajo en SparkR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de los datos\n",
    "\n",
    "Al igual que con los otros formatos, se pueden exportar a cualquier otro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Leemos un dataset que contiene los datos en formato Parquet\n",
    "df_parquet <- read.df(\"/SparkR/datasets/parquet/\", \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vemos la estructura del fichero y sus atributos\n",
    "printSchema(df_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vemos un resumen de los datos del fichero ...\n",
    "head(df_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hacemos un pequeño cambio en el nombre de las columnas del SparkDataFrame.\n",
    "colnames(df_parquet) <- c(\"user_id\",\"cat\",\"R1\",\"R2\",\"R3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos de nuevo el cambio de las columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(df_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Contamos los registros del dataset ... es pequeño, no es BigData...\n",
    "count(df_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos unas transformaciones sencillas al SparkDataFrame, copiando la tabla en una Vista Temporal para poder trabajar con ella en SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creamos una vista SparkDataFrame con el nombre \"tmp_parquet\".\n",
    "# Este nombre tmp_parquet es el nombre que se usará ahora.\n",
    "createOrReplaceTempView(df_parquet,\"tmp_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vista temporal, permite trabajar con una copia temporal de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contamos el número de registros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usamos SparkSQL para hacer consultas a los datos.\n",
    "count_rows <- sql(\"SELECT user_id,count(user_id) as registers FROM tmp_parquet group by user_id\")\n",
    "# Cuidado como obtener las cosas en SparkR: ---> Nooooooooo !!!! ;)\n",
    "# print(collect(count_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compara el tiempo la opción anterior y la siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(count_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usamos una vista temporal, está estará disponible durante toda la sesión a no ser que se elimine la vista temporal con `unpersist(....)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos con otro ejemplo, para saber las categorías que hay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# createOrReplaceTempView(df_parquet,\"tmp_parquet\") --> No volvermos a cargarla!\n",
    "# Usamos SparkSQL para hacer consultas a los datos.\n",
    "categories <- sql(\"SELECT cat FROM tmp_parquet group by cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo se calcularía el número de elementos de cada categoría?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# createOrReplaceTempView(df_parquet,\"tmp_parquet\")   --> No volvemos a cargarla !\n",
    "# Usamos SparkSQL para hacer consultas a los datos.\n",
    "categories_list <- sql(\"SELECT cat,count(user_id) as num_users FROM tmp_parquet group by cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(categories_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuántos usuarios distintos hay y qué suma total tienen por usuario?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# createOrReplaceTempView(df_parquet,\"tmp_parquet\") --> No volvemos a cargarla!\n",
    "# Usamos SparkSQL para hacer consultas a los datos.\n",
    "table_summary <- sql(\"SELECT user_id,SUM(R1) as sum_index FROM tmp_parquet group by user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count(table_summary)\n",
    "head(table_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Libreamos los recursos de la vista\n",
    "unpersist(table_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escritura de los datos\n",
    "\n",
    "Al igual que con los otros formatos, se pueden exportar a cualquier otro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Escritura del fichero de formato parquet a formato parquet\n",
    "write.df(finals, path = \"/SparkR/datasets/results/finals.parquet\", source = \"parquet\", mode = \"overwrite\")\n",
    "\n",
    "# Escritura del fichero de formato csv a formato a CSV\n",
    "write.df(finals, path = \"/SparkR/datasets/results/finals.csv\", source = \"csv\", mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON, acrónimo de JavaScript Object Notation, es un formato de texto ligero para el intercambio de datos. \n",
    "\n",
    "JSON es un subconjunto de la notación literal de objetos de JavaScript aunque hoy, debido a su amplia adopción como alternativa a XML, se considera un formato de lenguaje independiente.\n",
    "\n",
    "Es un formato actualmente muy utilizado ya que se ha impuesto como modelo para la entrada y salida de datos desde multiples y variados servicios web. Por ejemplo por citar varios:\n",
    "\n",
    "- Twitter. https://dev.twitter.com/rest/public \n",
    "- Google APIs\n",
    "- FaceBook API\n",
    "- ...\n",
    "\n",
    "Veamos en http://localhost:25980/tree/datasets como son los ficheros JSON por dentro.\n",
    "\n",
    "Es muy utilizado este formato para servicios web de información, donde lo que prima es la sencillez y versatilidad. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de datos\n",
    "\n",
    "La sintaxis es la misma, pero varía el identificador del tipo de fuente, en este caso JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "costumers <- read.df(\"/SparkR/datasets/json/buy_costumers_amazon.json\", \"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos la información del SparkDataFrame y su estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Un resumen\n",
    "head(costumers)\n",
    "\n",
    "# El numero de registros\n",
    "count(costumers)\n",
    "\n",
    "# La estructura\n",
    "printSchema(costumers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Por qué no se lee correctamente el JSON?** Revisamos el dataset http://localhost:25980/tree/datasets y arreglamos el error.\n",
    "\n",
    "La lectura de multiples ficheros es similar lo que ocurre con CSV, donde podemos indica que hay más archivos que queremos leer desde el directorio.\n",
    "\n",
    "Lectura desde varios ficheros JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "costumers_double <- read.json(c(\"/SparkR/datasets/json/buy_costumers_amazon.json\", \"/SparkR/datasets/json/buy_costumers_amazon.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuántos registros hay ahora?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count(costumers_double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Escritura de datos a formato JSON\n",
    "\n",
    "Al igual que para los otros formatos se usa el mismo esquema para guardar datasets a disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write.df(costumers_double, path = \"datasets/results/costumers.json\", source = \"json\", mode = \"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "<div style=\"font-family:helvetica;padding:5px;font-size:1.5em;background-color:#CFE7E2\">Ejercicio práctico:</div>\n",
    "\n",
    "Lee el dataset ``/SparkR/datasets/BNGhearth.dat`` que está en formato CSV y guardalo directamente en formato ``PARQUET```. Verifica que el nuevo ``dataset`` está en el formato correcto.\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre cerramos la sesion de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkR.session.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
