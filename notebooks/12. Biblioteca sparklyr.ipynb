{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller de procesamiento de BigData en Spark + R\n",
    "Manuel Parra (manuelparra@decsai.ugr.es). <a href=\"http://sci2s.ugr.es/es\">Soft Computing and Intelligent Information Systems</a>\n",
    ". <a href=\"http://sci2s.ugr.es/dicits/\">Distributed Computational Intelligence and Time Series</a>. **University of Granada**.\n",
    "![logos](https://sites.google.com/site/manuparra/home/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biblioteca ``sparklyr``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este paquete admite la conexión a clústeres con Apache Spark locales y remotos. Proporciona un backend compatible con \"dplyr\" y proporciona una interfaz para los algoritmos de aprendizaje de máquina incorporados en Spark.\n",
    "\n",
    "**Permite**\n",
    "\n",
    "* Conectar a Spark desde R. El paquete sparklyr proporciona un backend dplyr .\n",
    "* Filtrar y agrupar los datasets de Spark para el  análisis y visualización.\n",
    "* Utilizar la biblioteca de aprendizaje distribuida de Sparks de R.\n",
    "* Crear extensiones que llamen a la API Spark completa y proporcione interfaces con los paquetes Spark.\n",
    "\n",
    "\n",
    "Además junto con la interfaz ``dplyr`` de ``sparklyr``, puede crear y afinar fácilmente los flujos de trabajo de ML en Spark, orquestados dentro de R.\n",
    "\n",
    "Sparklyr proporciona tres familias de funciones que puede utilizar con el aprendizaje de máquina Spark:\n",
    "* Algoritmos de aprendizaje automático para el análisis de datos (funciones  ``ml_*``)\n",
    "* Transformadores de características para manipular características individuales (funciones ``ft_*``)\n",
    "* Funciones para manipular Spark DataFrames (funciones ``sdf_*``)\n",
    "\n",
    "<HR>\n",
    "\n",
    "Disponible:\n",
    "\n",
    "https://github.com/rstudio/sparklyr\n",
    "\n",
    "![logos](https://sites.google.com/site/manuparra/home/sparklyr-illustration.png)\n",
    "\n",
    "El paquete ``sparklyr`` proporciona una interfaz ``dplyr`` a **Spark DataFrames**, así como una interfaz R para los méodos  de ML de Spark.\n",
    "\n",
    "Spark es un sistema de computación de clusters de uso general, y existen muchas otras interfaces R que podrían ser implementadas (por ejemplo, interfaces con pipelines de ML, interfaces con paquetes de Spark de terceros, etc.).\n",
    "\n",
    "\n",
    "El flujo de trabajo para el análisis de datos con sparklyr podría estar compuesto de las siguientes etapas:\n",
    "* Realizar consultas SQL a través de la interfaz sparklyr dplyr,\n",
    "* Utilizar la familia de funciones sdf_ y ft_ para generar nuevas columnas o particionar su conjunto de datos,\n",
    "* Elegir un algoritmo de aprendizaje automático apropiado de la familia de funciones ml_ * para modelar los datos,\n",
    "* Inspeccionar la calidad del ajuste de su modelo y usarlo para hacer predicciones con nuevos datos,\n",
    "* Recopilar los resultados para la visualización y análisis posterior en R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialización del entorno con ``sparklyr``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario reiniciar Spark para poder trabajar con esta sesión de sparklyr     \n",
    "Para ello, ve a la Máquina Virtual para el proceso de Spark, para luego volver a cargarlo    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usamos la libreria sparklyr y dplyr.\n",
    "# Ajustar el nivel de visualización de errores !\n",
    "options(warn=0)\n",
    "\n",
    "# Incluimos la bilbioteca de sparklyr\n",
    "library(sparklyr)\n",
    "# Usamos la biblioteca para el manejo de los datos.\n",
    "library(dplyr)\n",
    "\n",
    "# Abrimos la conexión. Importante indicar la versión de Spark que tenemos instalada. En nuestro caso tenemos la 2.0.1\n",
    "sc <- spark_connect(master = \"local\", version = \"2.0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Características\n",
    "<HR>\n",
    "La biblioteca sparklyr tiene asociado un paquete que hace de complemento ideal para la manipulación de datos masivos. Este paquete es dplyr un paquete en R para trabajar con datos estructurados dentro y fuera de R. dplyr hace la manipulación de datos muy sencilla para los usuarios de R, además ofrece interfaces consistentes y con un buen rendimiento. La librería tiene las siguientes funcionalidades básicas:\n",
    "\n",
    "* Seleccion, filtrado y agregación.\n",
    "* Funciones para muestreo.\n",
    "* Funciones de JOIN para Dataframes.\n",
    "* Funciones Collect para transformar datos de Spark a R (importante!).\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura y escritura de datos con sparklyr\n",
    "<HR>\n",
    "\n",
    "Para la **lectura** de datos tenemos las siguientes funciones:\n",
    "\n",
    "* spark_read_csv: Lee un CSV y el resultado lo hace compatible con las funciones de dplyr.\n",
    "* spark_read_json: Lee un fichero JSON y el resultado es compatible con la interfaz de dplyr.\n",
    "* spark_read_parquet: Lee un fichero PARQUET.\n",
    "\n",
    "Además del formato de los datos, Spark soporta la lectura de datos desde una variedad de fuentes de datos. Estos incluyen, almacenamiento en \n",
    "\n",
    "* HDFS (hdfs:// protocol), \n",
    "* Amazon S3 (s3n:// protocol), o \n",
    "* ficheros locales disponibles en en los nodos (file:// protocol).\n",
    "\n",
    "Para la **escritura** de DataFrames existen las mismas funcione según el tipo de fuente de datos:\n",
    "\n",
    "* spark_write_csv: Escribe a CSV y recibe una fuente de datos compatible con dplyr.\n",
    "* spark_write_json: Escribe a JSON.\n",
    "* spark_write_parquet: Escribe a parquet desde cualquier fuente compatible con dplyr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lectura de un fichero de datos CSV\n",
    "\n",
    "tttm <- spark_read_csv(sc, \n",
    "                       name=\"tttm\", \n",
    "                       path=\"/SparkR/datasets/BNGhearth.dat\", \n",
    "                       delimiter = \",\", \n",
    "                       header=TRUE,\n",
    "                       overwrite = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Ha cargado los datos rápido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count(tttm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La escritura de datos es sencilla y simplemente requiere la función concreta para almacenar los datos.\n",
    "El valor del parámetros path puede ser de diferente origen de datos:\n",
    "* HDFS (``path=\"hdfs://....\"``)\n",
    "* AmazonS3 (``path=\"s3://...\"``)\n",
    "* Local (``path=\"...\"``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Escritura de un fichero de datos CSV (en local)\n",
    "spark_write_csv(tttm, \n",
    "                path=\"/SparkR/datasets/results/BNGhearth_RESULT.csv\", \n",
    "                delimiter = \",\", \n",
    "                header=TRUE)\n",
    "\n",
    "# Escritura de un fichero en S3 de Amazon \n",
    "# spark_write_csv(tttm, \n",
    "#                path=\"s3://mybucket/BNGhearth_RESULT.csv\", \n",
    "#                delimiter = \",\", \n",
    "#                header=TRUE)\n",
    "\n",
    "\n",
    "# Escritura de un fichero en HDFS \n",
    "# spark_write_csv(tttm, \n",
    "#                path=\"hdfs://user/manuelparra/BNGhearth_RESULT.csv\", \n",
    "#                delimiter = \",\", \n",
    "#                header=TRUE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los demás formatos, se usa la función correspondiente, teniendo en cuenta que la entrada de la función de escritura, siempre tiene que ser compatible con dplyr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spark_write_json(tttm, .....\n",
    "\n",
    "#spark_write_parquet(tttm, ....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrado, selección, agrupación.\n",
    "\n",
    "Las funciones de manejo de datos son comandos ``dplyr`` para manipular datos\n",
    "\n",
    "Cuando se conecta a un Spark DataFrame, ``dplyr`` traduce los comandos a las sentencias ``Spark SQL``. Las fuentes de datos remotas utilizan exactamente los mismos cinco verbos que las fuentes de datos locales. Aquí están los cinco verbos con sus comandos ``SQL`` correspondientes:\n",
    "\n",
    "- select ~ SELECT\n",
    "- filter ~ WHERE\n",
    "- arrange ~ ORDER\n",
    "- summarise ~ aggregators: sum, min, sd, etc.\n",
    "- mutate ~ operators: +, *, log, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Veamos la cabecera del dataset\n",
    "\n",
    "head(tttm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Seleccionamos las columnas que queremos con select\n",
    "\n",
    "select(tttm, age,sex,class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filtro los registros cuando sex y class son 1\n",
    "res_filtered<-filter(tttm, sex==1 & class==1)\n",
    "\n",
    "# Contamos los registros.\n",
    "count(res_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo es el dataset ``/SparkR/datasets/BNGhearth.dat``  : Balanceado o no Balanceado con respecto a la variable de clase ``class`` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res_class1 <- filter(tttm, class==1)\n",
    "count(res_class1)\n",
    "\n",
    "res_class0 <- filter(tttm, class==0)\n",
    "count(res_class0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro modo de saber si es o no balanceado de una forma más directa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Agrupamos el dataset por clase y luego contamos los registros.\n",
    "\n",
    "# En SQL sería select count(class) ...group by class\n",
    "\n",
    "# --> Importante, uso collect(...)\n",
    "num_regs <- as.integer(collect(count(tttm)))\n",
    "\n",
    "# Mostramos el número de registros\n",
    "print(num_regs)\n",
    "\n",
    "# Agrupamos por clase y contamos el numero de elementos de cada clase, ademñas \n",
    "# añadimos una columna que calcula el porcentaje que supone cada clase del total\n",
    "summarize( group_by(tttm,class), count = n(), percent= n()/num_regs *100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo es el dataset ``/SparkR/datasets/databig/ECBDL14_10tst.data`` : Balanceado o no Balanceado con respecto a la variable de clase ``class`` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Ejercicio \n",
    "tttm2 <- spark_read_csv(sc, \n",
    "                       name=\"tttm1\", \n",
    "                       path=\"/SparkR/datasets/databig/ECBDL14_10tst.data\", \n",
    "                       delimiter = \",\", \n",
    "                       header=TRUE,\n",
    "                       overwrite = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Ejercicio \n",
    "num_regs <- as.integer(collect(count(tttm2)))\n",
    "\n",
    "# Mostramos el número de registros\n",
    "print(num_regs)\n",
    "\n",
    "# Agrupamos por clase y contamos el numero de elementos de cada clase, ademñas \n",
    "# añadimos una columna que calcula el porcentaje que supone cada clase del total\n",
    "summarize( group_by(tttm2,class), count = n(), percent= n()/num_regs *100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesado de dataframes\n",
    "\n",
    "En ``sparklyr`` el interfaz de ``dplyr`` utiliza la SparkSQL para realizar los metodos de procesamiento de datos como agrupación, selección, mutacion, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(magrittr)\n",
    "\n",
    "# Atención collect (...)\n",
    "num_regs <- as.integer(collect(count(tttm)))\n",
    "\n",
    "# También podemos usar magittr para hacer los mismo de un modo más claro. El ejemplo de arriba y este son lo mismo.\n",
    "tttm %>% \n",
    "    group_by(class) %>%\n",
    "    summarize(count = n(), percent= n()/num_regs *100.0 )\n",
    "\n",
    "tttm %>% \n",
    "    group_by(class) %>%\n",
    "    summarize(count = n(), maxf1=max(f1),minf1= min(f1))\n",
    "\n",
    "# Imprimimos los primeros registros\n",
    "head(tttm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Añadimos una columna con el doble del valor de la columna f6\n",
    "tttm <- tttm %>%\n",
    "             mutate(chest2=chest*2.0)\n",
    "\n",
    "# Selecciono la columna chest2 que hemos creado recientemente.\n",
    "tttm %>% select(chest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Si del anterior ejemplo queremos construir un nuevo dataframe, usamos \n",
    "# select para tomar las columnas que nos interesen.\n",
    "tttm_new <- tttm %>%\n",
    "               select (age,sex, chest )\n",
    "\n",
    "head(tttm_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# La función arrange permite aplicar funciones de ordenación, ... sobre dataframes.\n",
    "tttm %>%\n",
    "    select (age,chest,bloodpressure ) %>%\n",
    "    arrange (desc(age)) %>%\n",
    "    filter (chest> 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Cómo sabemos cuantos objetos tenemos en el contexto de Spark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_tbls(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partición de un SparkDataFrame\n",
    "\n",
    "Permite la partición de un SparkDataFrame en varios grupos. Esta función es útil para dividir un DataFrame en, por ejemplo, conjuntos de datos de entrenamiento y prueba.\n",
    "\n",
    "````\n",
    "sdf_partition(x, ..., weights = NULL, seed = sample(.Machine$integer.max,  1))\n",
    "\n",
    "````\n",
    "\n",
    "Los pesos de muestreo definen la probabilidad de que una determinada observación se asignará a una partición en particular, no el tamaño resultante de la partición. Esto implica que particionar un DataFrame con, por ejemplo, sdf_partition (x, training = 0.5, test = 0.5)\n",
    "\n",
    "*Nota: No está garantizado para producir particiones de entrenamiento y prueba de igual tamaño.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partitions <- tttm %>%\n",
    "                sdf_partition(training = 0.6, test = 0.4)\n",
    "\n",
    "print(count(partitions$test))\n",
    "\n",
    "print(count(partitions$training))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizar predicciones sobre los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado un modelo ml_model junto a un nuevo conjunto de datos, producir un nuevo Spark DataFrame con valores predichos codificados en la columna \"predicción\".\n",
    "\n",
    "````\n",
    "sdf_predict(object, newdata, ...)\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de una columna\n",
    "\n",
    "Lee una sola columna de un SparkDataFrame y devuelva el contenido de esa columna en R.\n",
    "\n",
    "```\n",
    "sdf_read_column (x, columna)\n",
    "```\n",
    "\n",
    "*Devuelve el contenido en un objeto manipulable por R*.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registra un SparkDataFrame\n",
    "\n",
    "Regista un SparkDataFrame asignándole un nombre para la table en el contexto de SparkSQL.\n",
    "\n",
    "```\n",
    "sdf_register(x, name = NULL)\n",
    "\n",
    "```\n",
    "\n",
    "Permite usar ese DataFrame desde una consulta en SQL con Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de una muestra de un SparkDataFrame\n",
    "\n",
    "Extrae un muestreo aleatorio de filas de un SparkDataFrame\n",
    "\n",
    "```\n",
    "sdf_sample(x, fraction = 1, replacement = TRUE, seed = NULL)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Indico la fracción de la muestra y la semilla para la extracción.\n",
    "\n",
    "sdf_sample(tttm,fraction=0.2,seed=98765)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordenar un SparkDataFrame\n",
    "\n",
    "Ordena por una o varias columnas, con cada columna ordenada en orden ascendente.\n",
    "\n",
    "```\n",
    "sdf_sort(x, columns)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sdf_sort(tttm, c(\"age\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del dataset para Machine Learning\n",
    "\n",
    "Al tener un conjunto de datos grande y desbalanceado, podemos tomar varias alternativas para trabajar con el mismo:\n",
    "* Hacer un sobremuestreo de la clase minoritaria\n",
    "* Hacer un submuestreo de la clase mayoritaria\n",
    "\n",
    "Vamos a trabajar con los algoritmos de ML utilizando submuestreo Random UnderSampling sencillo, para ello:\n",
    "\n",
    "* Calculamos el número de instacias de la clase minoritaria.\n",
    "* Hacemos un muestreo sólo de la clase mayoritaria para igualar en instancias la clase minoritaria.\n",
    "* Fusionamos ambos muestreos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Contamos los registros de la clase minoritaria\n",
    "regs_minor <- tttm2 %>% \n",
    "            filter(class==1) %>% \n",
    "            count %>%\n",
    "            collect %>% \n",
    "            as.integer\n",
    "\n",
    "\n",
    "# Extraemos un sample con un numero similar de instancias que de la clase minoritaria\n",
    "only_class_0 <- tttm2 %>% \n",
    "        filter(class==0) %>%\n",
    "        sdf_sample(regs_minor, fraction=as.double(regs_minor/as.integer(collect(count(tttm)))))\n",
    "\n",
    "# Extraemos las instancias de la clase minoritaria \n",
    "only_class_1 <- tttm2 %>% \n",
    "            filter(class==1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contamos los registros de ambos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count(only_class_0)\n",
    "# as.integer(collect(count(only_class_0)))\n",
    "# only_class_0 %>% count %>% collect %>% as.integer\n",
    "count(only_class_1)\n",
    "# as.integer(collect(count(only_class_1)))\n",
    "# only_class_1 %>% count %>% collect %>% as.integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unimos los dos dataframes con rbind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unimos \n",
    "ds_ml <- rbind(only_class_1,only_class_0, name=\"ds_ml\")\n",
    "\n",
    "# Calculamos el tamaño de cada clase.\n",
    "ds_ml %>% \n",
    "        filter(class==0) %>%\n",
    "        count()\n",
    "\n",
    "ds_ml %>% \n",
    "        filter(class==1) %>%\n",
    "        count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el particionado usamos una función de sparklyr: sdf_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# La función sdf_partition devuelve el dataframe separado en training y test.\n",
    "# Para acceder a cada dataframe usamos ...$training , ...$test\n",
    "partitions <- sdf_partition(ds_ml,training=0.80,test=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Contamos el número de registros de cada conjunto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count(partitions$test)\n",
    "count(partitions$training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando SPARKSQL para el tratamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(DBI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultssql <- dbGetQuery(sc,\"select sex as sexo, age as edad from tttm LIMIT 10 \")\n",
    "print(resultssql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultssql <- dbGetQuery(sc,\"select count(class) as clases1 from tttm where class=1\")\n",
    "print(resultssql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultssql <- dbGetQuery(sc,\"select sex, count(class) as clases1 from tttm group by sex\")\n",
    "print(resultssql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultssql <- dbGetQuery(sc,\"select age from tttm where bloodpressure>=130 LIMIT 10\")\n",
    "print(resultssql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultssql <- dbGetQuery(sc,\"select age from tttm where bloodpressure>=130 order by age desc LIMIT 10\")\n",
    "print(resultssql)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
