{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller de procesamiento de BigData en Spark + R\n",
    "Manuel Parra (manuelparra@decsai.ugr.es). <a href=\"http://sci2s.ugr.es/es\">Soft Computing and Intelligent Information Systems</a>\n",
    ". <a href=\"http://sci2s.ugr.es/dicits/\">Distributed Computational Intelligence and Time Series</a>. **University of Granada**.\n",
    "![logos](https://sites.google.com/site/manuparra/home/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones sobre SparkDataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre para todos nuestros `scripts` con **SparkR**, cargamos la biblioteca, y creamos una nueva sesión de SparkR.\n",
    "\n",
    "En este caso:\n",
    "\n",
    "<span style=\"background-color:red;color:white\">&nbsp; &nbsp; Cuidado con la cantidad de MEMORIA que usamos para esta sección ! &nbsp; &nbsp; </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fijamos la ruta donde está instalado Spark\n",
    "Sys.setenv(\"SPARK_HOME\"='/usr/local/spark/')\n",
    "\n",
    ".libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"),\"R/lib/\"),.libPaths()))\n",
    "library(SparkR)\n",
    "sparkR.session(appName=\"EntornoInicio\", master = \"local[*]\", sparkConfig = list(spark.driver.memory = \"1g\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los ``SparkDataFrames`` soportan un alto número de funciones para hacer un procesado de datos estructurado. \n",
    "\n",
    "Vamos a poner en práctica las más utilizadas. \n",
    "\n",
    "**La lista completa de operaciones que se pueden aplicar se puede ver desde API de SparkR en https://spark.apache.org/docs/latest/api/R/index.html **\n",
    "\n",
    "![funcSparkR](https://sites.google.com/site/manuparra/home/functionSparkR.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones con SparkDataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cargamos una versión reducida de los datos en CSV\n",
    "df_nyctrips <- read.df(\"/SparkR/datasets/yellow_tripdata_2016-02_small1.csv\", \"csv\", header = \"true\", inferSchema = \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Probamos de nuevo sin INFERIR SCHEMA\n",
    "\n",
    "# Cargamos una versión reducida de los datos en CSV\n",
    "df_nyctrips <- read.df(\"/SparkR/datasets/yellow_tripdata_2016-02_small1.csv\", \"csv\", header = \"true\", inferSchema = \"false\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ¿Cuál de las dos sentencias anteriores ha tardado más? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estudiamos de manera superficial el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Comprobamos los campos del dataset\n",
    "printSchema(df_nyctrips)\n",
    "\n",
    "# Comprobamos como son los datos:\n",
    "head(df_nyctrips)\n",
    "\n",
    "# Contamos el total del registros:\n",
    "count(df_nyctrips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de instancias y columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la selección de columnas y filas, usamos ``select`` y ``filter``. \n",
    "\n",
    "Todas las operaciones se pueden combinar para producir un nuevo dataset o ``SparkDataFrame``. **Son equivalentes a usar SPARKSQL **.\n",
    "\n",
    "Estas operaciones son esenciales si queremos transformar el dataset en otra versión preprocesada del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Seleccionamos sólo la columna longitud, por el id de la columna\n",
    "# Por ID de columna \n",
    "head(select(df_nyctrips,df_nyctrips$pickup_longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Seleccionamos sólo la columna longitud, por el nombre de la columna.\n",
    "# Por nombre de columna del dataset\n",
    "head(select(df_nyctrips,\"pickup_longitude\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aplicar filtros de para las filas usamos ``filter`` que admite expresiones con operadores condicionales: \n",
    "\n",
    "```\n",
    "    < = > ! & | ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aplicamos un filtro para ver los viajes aquellos viajes de taxi de más de 10 millas.\n",
    "head(filter(df_nyctrips, df_nyctrips$trip_distance > 10 & df_nyctrips$total_amount> 20 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<HR>\n",
    "<div style=\"font-family:helvetica;padding:5px;font-size:1.5em;background-color:#CFE7E2\">Ejercicio práctico:</div>\n",
    "\n",
    "Selecciona todos los viajes que se hacen desde las 10 de la noche a las 6 de la mañana que tienen más de 3 pasajeros\n",
    "\n",
    "<HR>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aplicamos un filtro para ver los viajes aquellos viajes de taxi de más de 10 millas y el importe mayor de $ 20\n",
    "head(filter(df_nyctrips, df_nyctrips$trip_distance > 10 & df_nyctrips$total_amount> 20 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para agrupar datos se usa ``agg``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aplicamos un filtro para ver el viaje más caro en Taxi que se ha hecho:\n",
    "head( agg(df_nyctrips ,max = max(df_nyctrips$total_amount)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aplicamos un filtro para ver el viaje menos caro en Taxi que se ha hecho:\n",
    "head(agg(df_nyctrips, min = min(df_nyctrips$total_amount)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<HR>\n",
    "<div style=\"font-family:helvetica;padding:5px;font-size:1.5em;background-color:#CFE7E2\">Ejercicio práctico:</div>\n",
    "\n",
    "- Calcula cual es el viaje más largo que se ha hecho en kilometros (1 milla aprox 1.6 kilómetros).\n",
    "\n",
    "<HR>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de Agrupamiento y Agregación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los SparkDataFrames soportan funciones de agregado despues de agrupar:\n",
    "\n",
    "- ``groupBy``\n",
    "- ``summarize``\n",
    "\n",
    "Por ejemplo podemos utilizar lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Agrupamos por Vendedor y mostramos el número de viajes.\n",
    "head(summarize(groupBy(df_nyctrips, df_nyctrips$VendorID), count = n(df_nyctrips$VendorID)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Agrupamos por Vendedor y mostramos el número de viajes.\n",
    "head(summarize(groupBy(df_nyctrips, df_nyctrips$VendorID), max = max(df_nyctrips$total_amount)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Agrupamos y ordenamos\n",
    "\n",
    "numsum <- summarize(groupBy(df_nyctrips, df_nyctrips$VendorID), num = n(df_nyctrips$VendorID))\n",
    "head(arrange(numsum,asc(numsum$num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Agrupamos por numero de pasajeros y mostramos el numero de viajes\n",
    "trips_passenger <- summarize(groupBy(df_nyctrips, df_nyctrips$passenger_count), count = n(df_nyctrips$passenger_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cuidado con el COLLECT !\n",
    "trips_df <- head(collect(trips_passenger))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<HR>\n",
    "<div style=\"font-family:helvetica;padding:5px;font-size:1.5em;background-color:#CFE7E2\">Ejercicio práctico:</div>\n",
    "\n",
    "¿Qué ocurre si hacemos ``collect`` de un SparkDataFrame?\n",
    "\n",
    "\n",
    "<HR>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(trips_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones con columnas\n",
    "\n",
    "Otras operaciones en R, corresponden con la manipulación o transformación de valores en los registros de un dataset. En este caso la manipulación es muy sencilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convertimos la columna de millas a kilómetros, igual que en R.\n",
    "df_nyctrips$trip_distance <- df_nyctrips$trip_distance*1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(df_nyctrips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Añadir columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usamos mutate para añadir columnas que operan con elementos de las demás columnas.\n",
    "\n",
    "# mutate(sql_nyc,  uniform = rand(10),  normal  = randn(27))\n",
    "\n",
    "head(mutate(df_nyctrips,  uniform = rand(10),  normal  = randn(27)))\n",
    "head(mutate(df_nyctrips,  uniform =df_nyctrips$total_amount*1.1355,  normal  = randn(27)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<HR>\n",
    "<div style=\"font-family:helvetica;padding:5px;font-size:1.5em;background-color:#CFE7E2\">Pregunta</div>\n",
    "\n",
    "Añáde una columna que sea el tiempo del viaje. Pista ``INT(unix....())``.\n",
    "\n",
    "\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Otro modo de hacerlo es:\n",
    "\n",
    "head(withColumn(df_nyctrips,\"uniform\",rand(20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dapply -- dapplayCollect\n",
    "\n",
    "Aplicar una función a un conjunto datos masivo con ``dapply`` y ``dapplyCollect`` \n",
    "\n",
    "**dapply**\n",
    "\n",
    "Aplica una función a cada partición de un ``SparkDataFrame``. La función que será aplicada para cada partición y debería tener sólo un parámetro. La salida de la función deberá ser igualmente un data.frame. Además hay que especificar el ``schema`` del formato de los datos del ``SparkDataFrame`` resultante y deberá corresponder con tipo de datos del valor devuelto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hacemos una copia del SparkDataFrame para usarla en una vista temporal en SQL\n",
    "createOrReplaceTempView(df_nyctrips,\"slqdf_filtered_nyc\")\n",
    "\n",
    "# Hacemos una selección de los registros, donde calculamos el tiempo del viaje de cada viaje\n",
    "sql_nyc <- sql(\"select VendorID,INT(unix_timestamp(tpep_dropoff_datetime)- unix_timestamp(tpep_pickup_datetime)) AS trip_time,passenger_count,trip_distance,total_amount from slqdf_filtered_nyc\")\n",
    "\n",
    "# Mostramos un trozo de SparkDataFrame\n",
    "head(sql_nyc)\n",
    "\n",
    "schema(sql_nyc)\n",
    "\n",
    "# Indicamos el Schema, que debe coincidir con lo que queremos\n",
    "schema <- structType(\n",
    "    structField(\"VendorID\", \"integer\"),\n",
    "    structField(\"trip_time\", \"integer\"), \n",
    "    structField(\"passenger_count\", \"integer\"),\n",
    "    structField(\"trip_distance\", \"double\"),\n",
    "    structField(\"total_amount\", \"double\"),\n",
    "    structField(\"total_amount_euro\", \"double\")\n",
    ")\n",
    "\n",
    "# Creamos la función que hará los cambios.\n",
    "new_sql_nyc <- dapply(\n",
    "    sql_nyc, \n",
    "    function(x) { \n",
    "        x <- cbind(x, x$total_amount*1.1355) \n",
    "    }, \n",
    "    schema)\n",
    "\n",
    "# Vemos el cambio\n",
    "head(new_sql_nyc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### gapply -- gapplyCollect\n",
    "\n",
    "\n",
    "Aplica una función a cada uno de los grupos de un ``SparkDataFrame``. La función será aplicada a cada grupo del ``SparkDataFrame`` y debería tener sólo dos parámetros: agrupamiento por llave y data.frame al que corresponde esa llave. La salida de la función debería ser un data.frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Esquema del SparkDataFrame\n",
    "schema <- structType(\n",
    "    structField(\"VendorID\", \"integer\"),\n",
    "    structField(\"trip_time\", \"integer\"), \n",
    "    structField(\"passenger_count\", \"integer\"),\n",
    "    structField(\"trip_distance\", \"double\"),\n",
    "    structField(\"total_amount\", \"double\"),\n",
    "    structField(\"max_amount\", \"double\")\n",
    ")\n",
    "\n",
    "# Aplicamos la función gapply. Calculamos el máximo de cada Vendedor.\n",
    "result <- gapply(\n",
    "    sql_nyc,\n",
    "    \"VendorID\",\n",
    "    function(key, x) {\n",
    "        y <- data.frame(key, max(x$total_amount))\n",
    "    },\n",
    "    schema)\n",
    "\n",
    "# Mostramos el resultado.\n",
    "head(result[order(result$trip_distance, decreasing = TRUE), ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(sql_nyc)\n",
    "\n",
    "# Ahora probamos el gapplycollect: \n",
    "# Como el gapply, aplica una funcion a cada partición y luego hace un collect del resultado en un data.frame en R.\n",
    "result <- gapplyCollect(\n",
    "            \n",
    "    sql_nyc,\n",
    "    \"VendorID\",\n",
    "    function(key, x) {\n",
    "        y <- data.frame(key, max(x$trip_distance))\n",
    "        colnames(y) <- c(\"VendorID\", \"max_trip_distance\")\n",
    "        y\n",
    "    })\n",
    "\n",
    "# Vemos el resultado.\n",
    "head(result[order(result$trip_distance, decreasing = TRUE), ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operando con SparkSQL sobre cojuntos masivos de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las funciones de manejo de datos que se han usado con SparkR, pueden hacerse de una forma sencilla e intuitiva  con SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sql_nyc es nuestro DataFrameSpark de SQL\n",
    "createOrReplaceTempView(sql_nyc,\"slqdf_filtered_nyc\")\n",
    "\n",
    "# Hacemos una consulta para extraer el viaje de mayor distancia de cada venderor.\n",
    "results <- sql(\"select VendorID, MAX(trip_distance) from slqdf_filtered_nyc GROUP BY VendorID \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vemos el resultado.\n",
    "head(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recuerda cerrar la sesión con Spark**\n",
    "\n",
    "En secciones siguientes se revisará en profunidad SparkSQL."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
