{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller de procesamiento de BigData en Spark + R\n",
    "Manuel Parra (manuelparra@decsai.ugr.es). <a href=\"http://sci2s.ugr.es/es\">Soft Computing and Intelligent Information Systems</a>\n",
    ". <a href=\"http://sci2s.ugr.es/dicits/\">Distributed Computational Intelligence and Time Series</a>. **University of Granada**.\n",
    "![logos](https://sites.google.com/site/manuparra/home/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSets del Taller de SparkR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para todos los ejemplos y ejercicios del taller se ha habilitado una directorio en la Máquina Virtual con multiples conjuntos de datos de tipo variado (bioinformatica, logistica-transporte, salud).\n",
    "\n",
    "Los que vamos a utilizar serán los siguientes:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunto de datos: BNGheart\n",
    "\n",
    "Directorio en la Máquina Virtual:\n",
    "\n",
    "``/SparkR/datasets/BNGhearth.dat``\n",
    "\n",
    "Este conjunto de datos es una base de datos de enfermedades del corazón y contiene los siguientes atributos:\n",
    "\n",
    "- age \n",
    "- sex \n",
    "- chest pain type (4 values) \n",
    "- resting blood pressure \n",
    "- serum cholestoral in mg/dl \n",
    "- fasting blood sugar > 120 mg/dl \n",
    "- resting electrocardiographic results (values 0,1,2) \n",
    "- maximum heart rate achieved \n",
    "- exercise induced angina \n",
    "- oldpeak = ST depression induced by exercise relative to rest \n",
    "- the slope of the peak exercise ST segment \n",
    "- number of major vessels (0-3) colored by flourosopy \n",
    "- thal: 3 = normal; 6 = fixed defect; 7 = reversable defect \n",
    "\n",
    "Y la variable de clase: Ausencia o presencia  of problemas cardíacos\n",
    "\n",
    "\n",
    "## Conjunto de datos: ECBDL14\n",
    "\n",
    "Directorio en la Máquina Virtual:\n",
    "\n",
    "``/SparkR/datasets/databig/ECBDL14_10tst.data``\n",
    "\n",
    "Este conjunto de datos proviene del campo de predicción de estructuras de proteínas y se generó originalmente para crear un predictor para la predicción de contacto residuo-residuo de  CASP9. El conjunto de datos original tiene:\n",
    "\n",
    "- 32 millones de instancias, \n",
    "- 631 atributos, \n",
    "- 2 clases, \n",
    "- 98% de ejemplos negativos\n",
    "\n",
    "y ocupa, aproximadamente 56 GB de espacio en disco. Los detalles de la generación de conjuntos de datos y una estrategia de aprendizaje utilizados para entrenar un método para este problema usando computación evolutiva están disponibles en http://bioinformatics.oxfordjournals.org/content/28/19/2441.\n",
    "\n",
    "Training set (3723MB): <a href=\"http://cruncher.ncl.ac.uk/bdcomp/TrainingSet.arff.gz\">TrainingSet.arff.gz</a>\n",
    "Test set (347MB): <a href=\"http://cruncher.ncl.ac.uk/bdcomp/TestSet.arff.gz\">TestSet.arff.gz</a> \n",
    "\n",
    "Más información: http://cruncher.ncl.ac.uk/bdcomp/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Conjunto de datos: NYC YellowCab\n",
    "\n",
    "Directorio en la Máquina Virtual:\n",
    "\n",
    "``/SparkR/datasets/yellow_tripdata_2016-02_small1.csv``\n",
    "\n",
    "El dataset que vamos a usar para el procesamiento de dato masivos, corresponde con un conjunto de datos de los registros de viaje en TAXI, donde se capturan las fechas y horas de recogida y devolución de pasajeros, lugares de recogida y entrega (coordenadas), distancias de viaje, tarifas detalladas, tipos de tarifas, tipos de pago y conteos de pasajeros que van en el taxi.\n",
    "El dataset tiene MUCHAS posibilidades de procesamiento y también extracción de conocimiento.\n",
    "Estos conjuntos de datos adjuntos fueron recopilados y proporcionados por la Comisión de Taxisde Nueva York (TLC) http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml.\n",
    "\n",
    "\n",
    "** Características del conjunto de datos original:**\n",
    "\n",
    "![logosnyc](https://sites.google.com/site/manuparra/home/NYC.png)\n",
    "\n",
    "El conjunto de datos NYCTaxiTrips en total tiene sobre 267GB, que pueden ser manejados sin problema por SparkR (en un cluster real, no sobre una máquina virtual sencilla como la del taller).\n",
    "\n",
    "En total contiene 1100 millones de registros y se añaden más cada mes.\n",
    "\n",
    "Más información de como se gestionan 1100 millones de instancias en la siguiente web y se soluciona este problema problema real: http://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/\n",
    "\n",
    "Más datasets masivos de NYCTaxiTrips en: http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml\n",
    "\n",
    "Adicionalmente también están disponibles en ``/SparkR/datasets/`` :\n",
    "\n",
    "- ``yellow_tripdata_2016-01.csv``\n",
    "- ``yellow_tripdata_2016-02_small1.csv``\n",
    "- ``yellow_tripdata_2016-02_small2.csv``\n",
    "- ``yellow_tripdata_2016-02_small3.csv``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Como cargar estos conjuntos de datos en SparkR?\n",
    "\n",
    "La forma más sencilla de cargar los conjuntos de datos es usar las funciones de SparkR para leer conjuntos de datos en formato CSV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desde SparkR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cargamos los paths\n",
    ".libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"),\"R/lib/\"),.libPaths()))\n",
    "\n",
    "#Cargamos la libreria\n",
    "library(SparkR)\n",
    "\n",
    "#Abrimos la sesion con Spark\n",
    "sparkR.session(appName=\"EntornoInicio\",\n",
    "               master = \"local[*]\", \n",
    "               sparkConfig = list(spark.driver.memory = \"1g\"))\n",
    "\n",
    "# Cargamos por ejemplo el dataset de NYC\n",
    "df_nyctrips <- read.df(\"/root/TallerSparkR/datasets/yellow_tripdata_2016-02_small1.csv\", \n",
    "                           \"csv\", \n",
    "                           header = \"true\", \n",
    "                           inferSchema = \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desde sparklyr (no ejecutar aún)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# library(sparklyr)\n",
    "## Usamos la biblioteca para el manejo de los datos.\n",
    "#library(dplyr)\n",
    "\n",
    "## Abrimos la conexión. Importante indicar la versión de Spark que tenemos instalada. En nuestro caso tenemos la 2.0.1\n",
    "#sc <- spark_connect(master = \"local\", version = \"2.0.1\")\n",
    "\n",
    "\n",
    "## Lectura de un fichero de datos CSV\n",
    "\n",
    "#tttm <- spark_read_csv(sc, \n",
    "#                       name=\"tttm\", \n",
    "#                       path=\"/SparkR/datasets/BNGhearthBIG.dat\", \n",
    "#                       delimiter = \",\", \n",
    "#                       header=TRUE,\n",
    "#                       overwrite = TRUE)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
